{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"collapsed_sections":["fIYz2XcyxW3D","eZoDlnZ88Rei","PB0NDS0T8ISM","bHU0ZiSGCVL8","3hpBjfgnC5Ny","-9mGktRaSv6N","KalETRseiDp6","OJjpOFCKiDp7","aHTn3kL9iDp8","X6WLqPdDiDp8","vDEScR4oiDp-","iTpOzKCViDqH","wwV8Sl3LP6Dd","XKx09eemP6Dd","toafhiR0P6Dd","b7MOSkryP6De","0nGSOdsJP6Df","FkKR8ZScP6Dk"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"Wv4WxnrG8XtU"},"source":["#Collecting wikipedia articles on the occupations\n"]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"0oH-kbWkHgFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('//content//drive')"],"metadata":{"id":"SyPeJ5HLHqJv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wySjKu6E_wyW"},"source":["Here, we try to fetch the wikipedia article for each of the occupation to get relevant information for training. \n","None of the articles have any mentions of gendered pronoun"]},{"cell_type":"code","source":["pakoccupations=pd.read_csv(\"/content/drive/MyDrive/Pattern Recognition/GenderBiasCheckingDatasets/pakoccs2015.csv\")\n","banoccupations=pd.read_csv(\"/content/drive/MyDrive/Pattern Recognition/GenderBiasCheckingDatasets/banoccs2020.csv\")\n","afgoccupations=pd.read_csv(\"/content/drive/MyDrive/Pattern Recognition/GenderBiasCheckingDatasets/jobsACBAR.csv\")"],"metadata":{"id":"DHMzkV8nHaqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install wikipedia-api"],"metadata":{"id":"tysjsVJsG_Gf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77_W85Q_8WSL"},"outputs":[],"source":["import wikipediaapi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9IMdNqBD9Sw"},"outputs":[],"source":["wiki_wiki = wikipediaapi.Wikipedia(\n","        language='en',\n","        extract_format=wikipediaapi.ExtractFormat.WIKI\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IECGyi2zIgmH"},"outputs":[],"source":["banoccupations.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGp8D4oWJrZf"},"outputs":[],"source":["#banoccupations.insert(6, \"WikipediaArticle\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLaC7ro6DMhy"},"outputs":[],"source":["'''ind=0\n","for occupation in banoccupations[\"Unit group description (English)\"]:\n","  if banoccupations['WikipediaArticle'].values[ind]==\"\":\n","    p_wiki = wiki_wiki.page(occupation)\n","    banoccupations['WikipediaArticle'].values[ind]=p_wiki.text\n","  ind=ind+1'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1snA4FBMwtY"},"outputs":[],"source":["ind=0\n","filled=0\n","for occupation in banoccupations['WikipediaArticle']:\n","  if occupation==\"\":\n","    ind=ind+1\n","  else:\n","    filled=filled+1\n","print(ind, filled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzcwEK6yLNxv"},"outputs":[],"source":["banoccupations.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8pvG7RuOzzf"},"outputs":[],"source":["pakoccupations.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6kzOIF4PDs2"},"outputs":[],"source":["#pakoccupations.insert(6, \"WikipediaArticle\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKPS6l6iPUxv"},"outputs":[],"source":["'''ind=0\n","for occupation in pakoccupations[\"Unit group description (English)\"]:\n","  if pakoccupations['WikipediaArticle'].values[ind]==\"\":\n","    p_wiki = wiki_wiki.page(occupation)\n","    pakoccupations['WikipediaArticle'].values[ind]=p_wiki.text\n","  ind=ind+1'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucry3iAkPAa2"},"outputs":[],"source":["ind=0\n","filled=0\n","for occupation in pakoccupations['WikipediaArticle']:\n","  if occupation==\"\":\n","    ind=ind+1\n","  else:\n","    filled=filled+1\n","print(ind, filled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GSWnS5XKCEW"},"outputs":[],"source":["'''pakoccupations.insert(6, \"maleMentions\", 0)\n","pakoccupations.insert(7, \"femaleMentions\", 0)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyVUOHUJKWHe"},"outputs":[],"source":["'''ind=0\n","if \" He \" in pakoccupations[\"WikipediaArticle\"] or \" he \" in pakoccupations[\"WikipediaArticle\"] or \" He's \" in pakoccupations[\"WikipediaArticle\"] or \" he's \" in pakoccupations[\"WikipediaArticle\"] or \" Him \" in pakoccupations[\"WikipediaArticle\"] or \" him \" in pakoccupations[\"WikipediaArticle\"]:\n","  pakoccupations[\"maleMentions\"].values[ind]=1\n","if \" She \" in pakoccupations[\"WikipediaArticle\"] or \" she \" in pakoccupations[\"WikipediaArticle\"] or \" She's \" in pakoccupations[\"WikipediaArticle\"] or \" she's \" in pakoccupations[\"WikipediaArticle\"] or \" Her \" in pakoccupations[\"WikipediaArticle\"] or \" her \" in pakoccupations[\"WikipediaArticle\"]:\n","  pakoccupations[\"femaleMentions\"].values[ind]=1'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPnHMmerMOZe"},"outputs":[],"source":["pakoccupations.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2l2_I1NMHwG"},"outputs":[],"source":["'''males=0\n","females=0\n","for mention in pakoccupations['maleMentions']:\n","  males=males+mention\n","\n","for mention in pakoccupations['femaleMentions']:\n","  females=females+mention\n","\n","print(males, females)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEObA81YE-Rv"},"outputs":[],"source":["pakoccupations.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPOD4kCIZwyS"},"outputs":[],"source":["afgoccupations.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IC1WmxKGZuAp"},"outputs":[],"source":["#afgoccupations.insert(6, \"WikipediaArticle\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yDi6CYnZuAq"},"outputs":[],"source":["'''ind=0\n","for occupation in afgoccupations[\"JobPos\"]:\n","  p_wiki = wiki_wiki.page(occupation)\n","  afgoccupations['WikipediaArticle'].values[ind]=p_wiki.text\n","  ind=ind+1'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_o29ddoNZuAr"},"outputs":[],"source":["ind=0\n","filled=0\n","for occupation in afgoccupations['WikipediaArticle']:\n","  if occupation==\"\":\n","    ind=ind+1\n","  else:\n","    filled=filled+1\n","print(ind, filled)"]},{"cell_type":"markdown","source":["#Own model (With custom dataset)"],"metadata":{"id":"3kb6NjV1FtAS"}},{"cell_type":"code","source":["!pip install \"tensorflow-text>=2.11\"\n","!pip install einops"],"metadata":{"id":"qfHbMOo2v9Lv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","import typing\n","from typing import Any, Tuple\n","\n","import einops\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import tensorflow as tf\n","import tensorflow_text as tf_text"],"metadata":{"id":"tgvSaFVNwIgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","class ShapeChecker():\n","  def __init__(self):\n","    # Keep a cache of every axis-name seen\n","    self.shapes = {}\n","\n","  def __call__(self, tensor, names, broadcast=False):\n","    if not tf.executing_eagerly():\n","      return\n","\n","    parsed = einops.parse_shape(tensor, names)\n","\n","    for name, new_dim in parsed.items():\n","      old_dim = self.shapes.get(name, None)\n","      \n","      if (broadcast and new_dim == 1):\n","        continue\n","\n","      if old_dim is None:\n","        # If the axis name is new, add its length to the cache.\n","        self.shapes[name] = new_dim\n","        continue\n","\n","      if new_dim != old_dim:\n","        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n","                         f\"    found: {new_dim}\\n\"\n","                         f\"    expected: {old_dim}\\n\")"],"metadata":{"id":"SFnuSmLSwR2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Bengali"],"metadata":{"id":"fIYz2XcyxW3D"}},{"cell_type":"markdown","source":["###Getting the context-target pair"],"metadata":{"id":"eZoDlnZ88Rei"}},{"cell_type":"code","source":["def load_data(path):\n","  text = open(path, \"r\", encoding='utf-8')\n","  context=[]\n","  target=[]\n","  for line in text:\n","    pairs = line.split('\\t')\n","    context.append(pairs[1])\n","    target.append(pairs[0])\n","  context=np.array(context)\n","  target=np.array(target)\n","  return target, context"],"metadata":{"id":"XGc2AsFFwZFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targetRaw, contextRaw = load_data('/content/drive/MyDrive/Pattern Recognition/MultilingualCorpora/RandomNWiki/combinedWikiRandEn-Bn.txt')\n","print(contextRaw[-1])"],"metadata":{"id":"pnQXIyAd3r2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(targetRaw[-1])"],"metadata":{"id":"j5xNoRzG32yz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test Train split"],"metadata":{"id":"PB0NDS0T8ISM"}},{"cell_type":"code","source":["BUFFER_SIZE = len(contextRaw)\n","BATCH_SIZE = 64\n","\n","isTrain = np.random.uniform(size=(len(targetRaw),)) < 0.8\n","\n","trainRaw = (\n","    tf.data.Dataset\n","    .from_tensor_slices((contextRaw[isTrain], targetRaw[isTrain]))\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE))\n","val_raw = (\n","    tf.data.Dataset\n","    .from_tensor_slices((contextRaw[~isTrain], targetRaw[~isTrain]))\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE))"],"metadata":{"id":"3J0dH_gj7xQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for exampleContextStrings, exampleTargetStrings in trainRaw.take(1):\n","  print(exampleContextStrings[:5])\n","  print()\n","  print(exampleTargetStrings[:5])\n","  break"],"metadata":{"id":"X2hiF-PvB5V4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Text Preprocessing"],"metadata":{"id":"bHU0ZiSGCVL8"}},{"cell_type":"markdown","source":["###Standardization"],"metadata":{"id":"3hpBjfgnC5Ny"}},{"cell_type":"code","source":["exampleText = tf.constant('রুমে চলে যান।')\n","\n","print(exampleText.numpy())\n","print(tf_text.normalize_utf8(exampleText, 'NFKC').numpy())"],"metadata":{"id":"dc-e0v9ACYqq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unicode Normalization (For all languages)"],"metadata":{"id":"NKBTmbHKDZl6"}},{"cell_type":"code","source":["def lowerSplitPunct(text):\n","  # Split accented characters.\n","  text = tf_text.normalize_utf8(text, 'NFKC')\n","  text = tf.strings.lower(text)\n","  # Keep space, a to z, and select punctuation.\n","  text = tf.strings.regex_replace(text, '[^ \\u0980-\\u09FFa-z۔؟،«»।ا.?!,]', '')\n","  # Add spaces around punctuation.\n","  text = tf.strings.regex_replace(text, '[۔؟،«»ا।.?!,]', r' \\0 ')\n","  # Strip whitespace.\n","  text = tf.strings.strip(text)\n","\n","  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n","  return text"],"metadata":{"id":"qGQ1UpcNGW5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(exampleText.numpy().decode())\n","print(lowerSplitPunct(exampleText).numpy().decode())"],"metadata":{"id":"idnDeHlMFpKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Text Vectorization"],"metadata":{"id":"bwddulXVJbVm"}},{"cell_type":"code","source":["maxVocabSize = 5000\n","\n","contextTextProcessor = tf.keras.layers.TextVectorization(\n","    standardize=lowerSplitPunct,\n","    max_tokens=maxVocabSize,\n","    ragged=True)"],"metadata":{"id":"BWzb17YHJZ7X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train (adapt=~  Model.fit)"],"metadata":{"id":"-ORWOamDKt6V"}},{"cell_type":"code","source":["contextTextProcessor.adapt(trainRaw.map(lambda context, target: context))\n","\n","# Here are the first 10 words from the vocabulary:\n","contextTextProcessor.get_vocabulary()[:10]"],"metadata":{"id":"Jius-fwwK5SC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FOR ENGLISH"],"metadata":{"id":"CEzA04_mLzJJ"}},{"cell_type":"code","source":["targetTextProcessor = tf.keras.layers.TextVectorization(\n","    standardize=lowerSplitPunct,\n","    max_tokens=maxVocabSize,\n","    ragged=True)\n","\n","targetTextProcessor.adapt(trainRaw.map(lambda context, target: target))\n","targetTextProcessor.get_vocabulary()[:10]"],"metadata":{"id":"UNsoKZIrLgul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strings batch to tokens"],"metadata":{"id":"2p0Z70uBR2yU"}},{"cell_type":"code","source":["exampleTokens = contextTextProcessor(exampleContextStrings)\n","exampleTokens[:3, :]"],"metadata":{"id":"-yCvQcAOQU68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 2, 1)\n","plt.pcolormesh(exampleTokens.to_tensor())\n","plt.title('Token IDs')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(exampleTokens.to_tensor() != 0)\n","plt.title('Mask')"],"metadata":{"id":"HgLyeRVwSGjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Dataset processing"],"metadata":{"id":"-9mGktRaSv6N"}},{"cell_type":"code","source":["def processText(context, target):\n","  context = contextTextProcessor(context).to_tensor()\n","  target = targetTextProcessor(target)\n","  targIn = target[:,:-1].to_tensor()\n","  targOut = target[:,1:].to_tensor()\n","  return (context, targIn), targOut\n","\n","\n","trainDS = trainRaw.map(processText, tf.data.AUTOTUNE)\n","valDS = val_raw.map(processText, tf.data.AUTOTUNE)"],"metadata":{"id":"ms755wo7VE72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for (exContextTok, exTarIn), exTarOut in trainDS.take(1):\n","  print(exContextTok[0, :10].numpy()) \n","  print()\n","  print(exTarIn[0, :10].numpy()) \n","  print(exTarOut[0, :10].numpy()) "],"metadata":{"id":"87tgRmD1WTaV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder-Decoder"],"metadata":{"id":"D6hH_37_Xzib"}},{"cell_type":"code","source":["UNITS = 400"],"metadata":{"id":"lL_0-KaQX1wu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder"],"metadata":{"id":"osfDR07-X-1C"}},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, text_processor, units):\n","    super(Encoder, self).__init__()\n","    self.text_processor = text_processor\n","    self.vocab_size = text_processor.vocabulary_size()\n","    self.units = units\n","    \n","    # The embedding layer converts tokens to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n","                                               mask_zero=True)\n","\n","    # The RNN layer processes those vectors sequentially.\n","    self.rnn = tf.keras.layers.Bidirectional(\n","        merge_mode='mul',\n","        layer=tf.keras.layers.GRU(units,\n","                            # Return the sequence and state\n","                            return_sequences=True,\n","                            recurrent_initializer='glorot_uniform'))\n","\n","  def call(self, x):\n","    shape_checker = ShapeChecker()\n","    shape_checker(x, 'batch s')\n","\n","    # 2. The embedding layer looks up the embedding vector for each token.\n","    x = self.embedding(x)\n","    shape_checker(x, 'batch s units')\n","\n","    # 3. The GRU processes the sequence of embeddings.\n","    x = self.rnn(x)\n","    shape_checker(x, 'batch s units')\n","\n","    # 4. Returns the new sequence of embeddings.\n","    return x\n","\n","  def convert_input(self, texts):\n","    texts = tf.convert_to_tensor(texts)\n","    if len(texts.shape) == 0:\n","      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n","    context = self.text_processor(texts).to_tensor()\n","    context = self(context)\n","    return context"],"metadata":{"id":"ML2f-r_YX-iK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder check"],"metadata":{"id":"SQiBmpKSvsGi"}},{"cell_type":"code","source":["# Encode the input sequence.\n","encoder = Encoder(contextTextProcessor, UNITS)\n","exContext = encoder(exContextTok)\n","\n","print(f'Context tokens, shape (batch, s): {exContextTok.shape}')\n","print(f'Encoder output, shape (batch, s, units): {exContext.shape}')"],"metadata":{"id":"G2Umhd6fY04P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention Layer"],"metadata":{"id":"iokuURUWfh3B"}},{"cell_type":"code","source":["class CrossAttention(tf.keras.layers.Layer):\n","  def __init__(self, units, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=2, **kwargs)\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","    self.add = tf.keras.layers.Add()\n","\n","  def call(self, x, context):\n","    shape_checker = ShapeChecker()\n"," \n","    shape_checker(x, 'batch t units')\n","    shape_checker(context, 'batch s units')\n","\n","    attn_output, attn_scores = self.mha(\n","        query=x,\n","        value=context,\n","        return_attention_scores=True)\n","    \n","    shape_checker(x, 'batch t units')\n","    shape_checker(attn_scores, 'batch heads t s')\n","    \n","    # Cache the attention scores for plotting later.\n","    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n","    shape_checker(attn_scores, 'batch t s')\n","    self.last_attention_weights = attn_scores\n","\n","    x = self.add([x, attn_output])\n","    x = self.layernorm(x)\n","\n","    return x"],"metadata":{"id":"omHGTav5fkOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionLayer = CrossAttention(UNITS)\n","\n","# Attend to the encoded tokens\n","embed = tf.keras.layers.Embedding(targetTextProcessor.vocabulary_size(),\n","                                  output_dim=UNITS, mask_zero=True)\n","exTarEmbed = embed(exTarIn)\n","\n","result = attentionLayer(exTarEmbed, exContext)\n","\n","print(f'Context sequence, shape (batch, s, units): {exContext.shape}')\n","print(f'Target sequence, shape (batch, t, units): {exTarEmbed.shape}')\n","print(f'Attention result, shape (batch, t, units): {result.shape}')\n","print(f'Attention weights, shape (batch, t, s):    {attentionLayer.last_attention_weights.shape}')"],"metadata":{"id":"dGji0VOFf7zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionLayer.last_attention_weights[0].numpy().sum(axis=-1)"],"metadata":{"id":"FiRI_7VXgdpJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionWeights = attentionLayer.last_attention_weights\n","mask=(exContextTok != 0).numpy()\n","\n","plt.subplot(1, 2, 1)\n","plt.pcolormesh(mask*attentionWeights[:, 0, :])\n","plt.title('Attention weights')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(mask)\n","plt.title('Mask');\n"],"metadata":{"id":"UG7eSibpguIw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decoder"],"metadata":{"id":"g0qMRDbKjGoQ"}},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  @classmethod\n","  def add_method(cls, fun):\n","    setattr(cls, fun.__name__, fun)\n","    return fun\n","\n","  def __init__(self, text_processor, units):\n","    super(Decoder, self).__init__()\n","    self.text_processor = text_processor\n","    self.vocab_size = text_processor.vocabulary_size()\n","    self.word_to_id = tf.keras.layers.StringLookup(\n","        vocabulary=text_processor.get_vocabulary(),\n","        mask_token='', oov_token='[UNK]')\n","    self.id_to_word = tf.keras.layers.StringLookup(\n","        vocabulary=text_processor.get_vocabulary(),\n","        mask_token='', oov_token='[UNK]',\n","        invert=True)\n","    self.start_token = self.word_to_id('[START]')\n","    self.end_token = self.word_to_id('[END]')\n","\n","    self.units = units\n","\n","\n","    # 1. The embedding layer converts token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n","                                               units, mask_zero=True)\n","\n","    # 2. The RNN keeps track of what's been generated so far.\n","    self.rnn = tf.keras.layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","    # 3. The RNN output will be the query for the attention layer.\n","    self.attention = CrossAttention(units)\n","\n","    # 4. This fully connected layer produces the logits for each\n","    # output token.\n","    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"],"metadata":{"id":"oGgnRIY2jKAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@Decoder.add_method\n","def call(self,\n","         context, x,\n","         state=None,\n","         return_state=False):  \n","  shape_checker = ShapeChecker()\n","  shape_checker(x, 'batch t')\n","  shape_checker(context, 'batch s units')\n","\n","  # 1. Lookup the embeddings\n","  x = self.embedding(x)\n","  shape_checker(x, 'batch t units')\n","\n","  # 2. Process the target sequence.\n","  x, state = self.rnn(x, initial_state=state)\n","  shape_checker(x, 'batch t units')\n","\n","  # 3. Use the RNN output as the query for the attention over the context.\n","  x = self.attention(x, context)\n","  self.last_attention_weights = self.attention.last_attention_weights\n","  shape_checker(x, 'batch t units')\n","  shape_checker(self.last_attention_weights, 'batch t s')\n","\n","  # Step 4. Generate logit predictions for the next token.\n","  logits = self.output_layer(x)\n","  shape_checker(logits, 'batch t target_vocab_size')\n","\n","  if return_state:\n","    return logits, state\n","  else:\n","    return logits"],"metadata":{"id":"jzAhwPH2jQi5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(targetTextProcessor, UNITS)"],"metadata":{"id":"Vnlp8sLljnhv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given the context and target tokens, for each target token it predicts the next target token."],"metadata":{"id":"buj4rC_1kbVY"}},{"cell_type":"markdown","source":["Decoder check"],"metadata":{"id":"EYOQAlcvvvnC"}},{"cell_type":"code","source":["logits = decoder(exContext, exTarIn)\n","\n","print(f'encoder output shape: (batch, s, units) {exContext.shape}')\n","print(f'input target tokens shape: (batch, t) {exTarIn.shape}')\n","print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"],"metadata":{"id":"iDP9IRTrkLgH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["INFERENCE"],"metadata":{"id":"rncniz07kya_"}},{"cell_type":"code","source":["@Decoder.add_method\n","def get_initial_state(self, context):\n","  batch_size = tf.shape(context)[0]\n","  start_tokens = tf.fill([batch_size, 1], self.start_token)\n","  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","  embedded = self.embedding(start_tokens)\n","  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"],"metadata":{"id":"HTc9KLqHk1p_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@Decoder.add_method\n","def tokens_to_text(self, tokens):\n","  words = self.id_to_word(tokens)\n","  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n","  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n","  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n","  return result"],"metadata":{"id":"XcQDirbRk4io"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@Decoder.add_method\n","def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n","  logits, state = self(\n","    context, next_token,\n","    state = state,\n","    return_state=True) \n","  \n","  if temperature == 0.0:\n","    next_token = tf.argmax(logits, axis=-1)\n","  else:\n","    logits = logits[:, -1, :]/temperature\n","    next_token = tf.random.categorical(logits, num_samples=1)\n","\n","  # If a sequence produces an `end_token`, set it `done`\n","  done = done | (next_token == self.end_token)\n","  # Once a sequence is done it only produces 0-padding.\n","  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n","  \n","  return next_token, done, state"],"metadata":{"id":"uvhysOjWk8oQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generation Loop"],"metadata":{"id":"qIVrEfZtlHaw"}},{"cell_type":"code","source":["# Setup the loop variables.\n","nextToken, done, state = decoder.get_initial_state(exContext)\n","tokens = []\n","\n","for n in range(10):\n","  # Run one step.\n","  next_token, done, state = decoder.get_next_token(\n","      exContext, nextToken, done, state, temperature=1.0)\n","  # Add the token to the output.\n","  tokens.append(next_token)\n","\n","# Stack all the tokens together.\n","tokens = tf.concat(tokens, axis=-1) # (batch, t)\n","\n","# Convert the tokens back to a a string\n","result = decoder.tokens_to_text(tokens)\n","result[:3].numpy()"],"metadata":{"id":"5Lyt1YnFlJj-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actual Model"],"metadata":{"id":"5_YpOoNTluLw"}},{"cell_type":"code","source":["class Translator(tf.keras.Model):\n","  @classmethod\n","  def add_method(cls, fun):\n","    setattr(cls, fun.__name__, fun)\n","    return fun\n","\n","  def __init__(self, units,\n","               context_text_processor,\n","               target_text_processor):\n","    super().__init__()\n","    # Build the encoder and decoder\n","    encoder = Encoder(context_text_processor, units)\n","    decoder = Decoder(target_text_processor, units)\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","\n","  def call(self, inputs):\n","    context, x = inputs\n","    context = self.encoder(context)\n","    logits = self.decoder(context, x)\n","\n","    #TODO(b/250038731): remove this\n","    try:\n","      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n","      del logits._keras_mask\n","    except AttributeError:\n","      pass\n","\n","    return logits"],"metadata":{"id":"qS5t3Dislvqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Translator(UNITS, contextTextProcessor, targetTextProcessor)\n","\n","logits = model((exContextTok, exTarIn))\n","\n","print(f'Context tokens, shape: (batch, s, units) {exContextTok.shape}')\n","print(f'Target tokens, shape: (batch, t) {exTarIn.shape}')\n","print(f'logits, shape: (batch, t, targetVocabularySize) {logits.shape}')"],"metadata":{"id":"lBJ-WtX3lzWI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"-E_GT_GXmHjU"}},{"cell_type":"markdown","source":["loss function"],"metadata":{"id":"Y3Wvhp4Fw36B"}},{"cell_type":"code","source":["def masked_loss(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    loss = loss_fn(y_true, y_pred)\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, loss.dtype)\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"],"metadata":{"id":"zghp7xftmI3J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["accuracy function"],"metadata":{"id":"mTACCXQ-w665"}},{"cell_type":"code","source":["def masked_acc(y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    y_pred = tf.argmax(y_pred, axis=-1)\n","    y_pred = tf.cast(y_pred, y_true.dtype)\n","    \n","    match = tf.cast(y_true == y_pred, tf.float32)\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    \n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)"],"metadata":{"id":"k8uSW-dSmVIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model Config with \n","Adam optimizer and metrics"],"metadata":{"id":"2v9JfzrHmaoe"}},{"cell_type":"code","source":["model.compile(optimizer='adam',\n","              loss=masked_loss, \n","              metrics=[masked_acc, masked_loss])"],"metadata":{"id":"_8crugNJmZGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabSize = 1.0 * targetTextProcessor.vocabulary_size()\n","\n","{\"expected_loss\": tf.math.log(vocabSize).numpy(),\n"," \"expected_acc\": 1/vocabSize}"],"metadata":{"id":"xLAVhg06mn-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(valDS, steps=20, return_dict=True)"],"metadata":{"id":"M18o9K6tnAC2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TRAINING THE MODEL (FINALLY)"],"metadata":{"id":"UynyyLUinqPZ"}},{"cell_type":"code","source":["history = model.fit(\n","    trainDS.repeat(), \n","    epochs=40,\n","    steps_per_epoch = 130,\n","    validation_data=valDS,\n","    validation_steps = 20,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(patience=3)])"],"metadata":{"id":"MJJoRDobnSWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'], label='loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"],"metadata":{"id":"uWUlFoc5nx6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['masked_acc'], label='accuracy')\n","plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"],"metadata":{"id":"PjFiLZwGn0bo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Translate"],"metadata":{"id":"T6a-n3aOVeId"}},{"cell_type":"code","source":["#@title\n","@Translator.add_method\n","def translate(self,\n","              texts, *,\n","              max_length=50,\n","              temperature=0.0):\n","  # Process the input texts\n","  context = self.encoder.convert_input(texts)\n","  batch_size = tf.shape(texts)[0]\n","\n","  # Setup the loop inputs\n","  tokens = []\n","  attention_weights = []\n","  next_token, done, state = self.decoder.get_initial_state(context)\n","\n","  for _ in range(max_length):\n","    # Generate the next token\n","    next_token, done, state = self.decoder.get_next_token(\n","        context, next_token, done,  state, temperature)\n","        \n","    # Collect the generated tokens\n","    tokens.append(next_token)\n","    attention_weights.append(self.decoder.last_attention_weights)\n","    \n","    if tf.executing_eagerly() and tf.reduce_all(done):\n","      break\n","\n","  # Stack the lists of tokens and attention weights.\n","  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n","  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n","\n","  result = self.decoder.tokens_to_text(tokens)\n","  return result"],"metadata":{"id":"6sDRFyroVij-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = model.translate(['তিনি একজন আইনজীবী'])\n","result[0].numpy().decode()"],"metadata":{"id":"d7xl_Gs2Vk2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = model.translate(['সে একজন ডাক্তার হবে'])\n","result[0].numpy().decode()"],"metadata":{"id":"QXkhLrQon1AN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result = model.translate(['তিনি একজন পাইলট'])\n","result[0].numpy().decode()"],"metadata":{"id":"4Gmfnn-3ohnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result = model.translate(['তিনি একজন প্রশিক্ষক'])\n","result[0].numpy().decode()"],"metadata":{"id":"iXwGa34Sox5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result = model.translate(['গতকাল সেই ব্যক্তিকে দেখেছি'])\n","result[0].numpy().decode()"],"metadata":{"id":"A1mEcanMpF0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result = model.translate(['গতকাল সেই লোকটিকে দেখেছি। সে একজন ডাক্তার.'])\n","result[0].numpy().decode()"],"metadata":{"id":"VaXamuT6j32S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5hQWlbN3jGF"},"outputs":[],"source":["#@title\n","@Translator.add_method\n","def plot_attention(self, text, **kwargs):\n","  assert isinstance(text, str)\n","  output = self.translate([text], **kwargs)\n","  output = output[0].numpy().decode()\n","\n","  attention = self.last_attention_weights[0]\n","\n","  context = lowerSplitPunct(text)\n","  context = context.numpy().decode().split()\n","\n","  output = lowerSplitPunct(output)\n","  output = output.numpy().decode().split()[1:]\n","\n","  fig = plt.figure(figsize=(10, 10))\n","  ax = fig.add_subplot(1, 1, 1)\n","\n","  ax.matshow(attention, cmap='viridis', vmin=0.0)\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + output, fontdict=fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  ax.set_xlabel('Input text')\n","  ax.set_ylabel('Output text')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrGawQv2eiA4"},"outputs":[],"source":["model.plot_attention('তিনি একজন আইনজীবী') # Are you still home"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flT0VlQZK11s"},"outputs":[],"source":["%%time\n","# This is my life.\n","model.plot_attention('এটাই আমার জীবন.')"]},{"cell_type":"markdown","source":["###Bias Performance check"],"metadata":{"id":"dGkHme7S4cSf"}},{"cell_type":"code","source":["banoccupations=banoccupations.drop('WikipediaArticle', axis=1)"],"metadata":{"id":"_zvzcOyF8Wf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations.head()"],"metadata":{"id":"pcd-9UEW70Tm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations.columns"],"metadata":{"id":"VjsGlfKh8Qnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations.insert(10, \"ModelTranslation(SingleSentence)\", \"\")"],"metadata":{"id":"SjLjA_rj-ErN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for sent in banoccupations['Sent(Bn)']:\n","  result = model.translate([sent])\n","  result=result[0].numpy().decode()\n","  banoccupations['ModelTranslation(SingleSentence)'].values[ind]=result\n","  ind=ind+1"],"metadata":{"id":"F7Aoy4Cl-tKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations.insert(10, \"ModelTranslation(MultipleSentences)\", \"\")"],"metadata":{"id":"ZYJ7VDhg1Hu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for sent in banoccupations['SentWithCont(Bn)']:\n","  result = model.translate([sent])\n","  result=result[0].numpy().decode()\n","  banoccupations['ModelTranslation(MultipleSentences)'].values[ind]=result\n","  ind=ind+1"],"metadata":{"id":"aeZEVfUm1YKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations.insert(10, \"ModelPronoun(SingleSentence)\", \"\")"],"metadata":{"id":"yw2teHeq3nM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for val in banoccupations['ModelTranslation(SingleSentence)']:\n","  #print(ind, val)\n","  if (' She ' in val) or ((\" She's \" in val)) or (' she ' in val) or ((\" she's \" in val)):\n","    banoccupations['ModelPronoun(SingleSentence)'].values[ind]='F'\n","  elif (' He ' in val) or ((\" He's \" in val)) or (' he ' in val) or ((\" he's \" in val)):\n","    banoccupations['ModelPronoun(SingleSentence)'].values[ind]='M'\n","  else:\n","    banoccupations['ModelPronoun(SingleSentence)'].values[ind]='O'\n","  ind=ind+1"],"metadata":{"id":"IRDcxrJI3Uro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations.insert(10, \"ModelPronoun(MultipleSentences)\", \"\")"],"metadata":{"id":"kux-OC76AO01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for val in banoccupations['ModelTranslation(SingleSentence)']:\n","  #print(ind, val)\n","  if ('She ' in val) or ((\"She's \" in val)) or ('she ' in val) or ((\"she's \" in val)):\n","    banoccupations['ModelPronoun(SingleSentence)'].values[ind]='F'\n","  elif ('He ' in val) or ((\"He's \" in val)) or ('he ' in val) or ((\"he's \" in val)):\n","    banoccupations['ModelPronoun(SingleSentence)'].values[ind]='M'\n","  else:\n","    banoccupations['ModelPronoun(SingleSentence)'].values[ind]='O'\n","  ind=ind+1"],"metadata":{"id":"cOHoVrNZAEqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def autopct(pct): # only show the label when it's > 10%\n","    return ('%.2f' % pct) if pct > 10 else ''\n","'''labels=('Female', 'Male', 'Others')\n","afgoccupations_ax=afgoccupations['Pronoun(GTranslateAlt)'].value_counts().plot(kind='pie', figsize=(13,8), autopct=autopct, title='Male, Female and Other pronouns count in the Alternative single sentence translation by Google Translate', labels=None)\n","afgoccupations_ax.axes.get_yaxis().set_visible(False)\n","plt.legend(loc=5, labels=labels)'''"],"metadata":{"id":"Zjx8K0EN4Xmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations_ax=banoccupations['ModelPronoun(SingleSentence)'].value_counts().plot(kind='pie', autopct=autopct, title='Male, Female and Other pronouns count in the translation without context(Bangla)')\n","plt.show()"],"metadata":{"id":"ABJ-9kwA4Rxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations['ModelPronoun(SingleSentence)'].value_counts()"],"metadata":{"id":"Kowq1WQe3l63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations_ax=banoccupations['ModelPronoun(MultipleSentences)'].value_counts().plot(kind='pie', autopct=autopct, title='Male, Female and Other pronouns count in the translation with context(Bangla)')\n","plt.show()"],"metadata":{"id":"oHC_787cAn-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["banoccupations['ModelPronoun(MultipleSentences)'].value_counts()"],"metadata":{"id":"XeoH56tvAqyk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Urdu"],"metadata":{"id":"KalETRseiDp6"}},{"cell_type":"markdown","source":["###Getting the context-target pair"],"metadata":{"id":"OJjpOFCKiDp7"}},{"cell_type":"code","source":["targetRaw, contextRaw = load_data('/content/drive/MyDrive/Pattern Recognition/MultilingualCorpora/RandomNWiki/combinedWikiRandEn-Ur.txt')\n","print(contextRaw[25])"],"metadata":{"id":"6lJSoQimiDp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(targetRaw[25])"],"metadata":{"id":"HB-CnbdviDp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test Train split"],"metadata":{"id":"aHTn3kL9iDp8"}},{"cell_type":"code","source":["BUFFER_SIZE = len(contextRaw)\n","BATCH_SIZE = 64\n","\n","isTrain = np.random.uniform(size=(len(targetRaw),)) < 0.8\n","\n","trainRaw = (\n","    tf.data.Dataset\n","    .from_tensor_slices((contextRaw[isTrain], targetRaw[isTrain]))\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE))\n","valRaw = (\n","    tf.data.Dataset\n","    .from_tensor_slices((contextRaw[~isTrain], targetRaw[~isTrain]))\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE))"],"metadata":{"id":"j5j0cZLBiDp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for exampleContextStrings, exampleTargetStrings in trainRaw.take(1):\n","  print(exampleContextStrings[:5])\n","  print()\n","  print(exampleTargetStrings[:5])\n","  break"],"metadata":{"id":"9D_GOLs9iDp8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Text Preprocessing"],"metadata":{"id":"z4A08ObPiDp8"}},{"cell_type":"markdown","source":["###Standardization"],"metadata":{"id":"X6WLqPdDiDp8"}},{"cell_type":"code","source":["exampleText = tf.constant('آدمی')\n","\n","print(exampleText.numpy())\n","print(tf_text.normalize_utf8(exampleText, 'NFKC').numpy())"],"metadata":{"id":"XLkl20BWiDp8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unicode Normalization (For all languages)"],"metadata":{"id":"13rzJj1oiDp8"}},{"cell_type":"code","source":["print(exampleText.numpy().decode())\n","print(lowerSplitPunct(exampleText).numpy().decode())"],"metadata":{"id":"nDb5AHLOiDp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Text Vectorization"],"metadata":{"id":"3Z-Auq-kiDp9"}},{"cell_type":"markdown","source":["Train (adapt=~  Model.fit)"],"metadata":{"id":"o_J2RJ3xiDp9"}},{"cell_type":"code","source":["contextTextProcessor.adapt(trainRaw.map(lambda context, target: context))\n","\n","# Here are the first 10 words from the vocabulary:\n","contextTextProcessor.get_vocabulary()[:50]"],"metadata":{"id":"lssRMeeAiDp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FOR ENGLISH"],"metadata":{"id":"yORIrjQEiDp9"}},{"cell_type":"code","source":["targetTextProcessor.adapt(trainRaw.map(lambda context, target: target))\n","targetTextProcessor.get_vocabulary()[:10]"],"metadata":{"id":"ef3kAmf1iDp-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strings batch to tokens"],"metadata":{"id":"BQPc9EPBiDp-"}},{"cell_type":"code","source":["exampleTokens = contextTextProcessor(exampleContextStrings)\n","exampleTokens[:3, :]"],"metadata":{"id":"thPqycYIiDp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 2, 1)\n","plt.pcolormesh(exampleTokens.to_tensor())\n","plt.title('Token IDs')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(exampleTokens.to_tensor() != 0)\n","plt.title('Mask')"],"metadata":{"id":"8WoBJbhuiDp-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Dataset processing"],"metadata":{"id":"vDEScR4oiDp-"}},{"cell_type":"markdown","source":["Vectorization"],"metadata":{"id":"UijZ284B6C3i"}},{"cell_type":"code","source":["trainDS= trainRaw.map(processText, tf.data.AUTOTUNE)\n","valDS= valRaw.map(processText, tf.data.AUTOTUNE)"],"metadata":{"id":"zEbOj6fjiDp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for (exContextTok, exTarIn), exTarOut in trainDS.take(1):\n","  print(exContextTok[0, :10].numpy()) \n","  print()\n","  print(exTarIn[0, :10].numpy()) \n","  print(exTarOut[0, :10].numpy()) "],"metadata":{"id":"CARfKEVTiDp-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder-Decoder"],"metadata":{"id":"40iCJhTliDp_"}},{"cell_type":"markdown","source":["Encoder check"],"metadata":{"id":"I-uM2vHAiDp_"}},{"cell_type":"code","source":["# Encode the input sequence.\n","encoder = Encoder(contextTextProcessor, UNITS)\n","exContext = encoder(exContextTok)\n","\n","print(f'Context tokens, shape (batch, s): {exContextTok.shape}')\n","print(f'Encoder output, shape (batch, s, units): {exContext.shape}')"],"metadata":{"id":"pFWEgzU0iDp_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention Layer"],"metadata":{"id":"zt_m6-aYiDp_"}},{"cell_type":"code","source":["attentionLayer = CrossAttention(UNITS)\n","\n","# Attend to the encoded tokens\n","embed = tf.keras.layers.Embedding(targetTextProcessor.vocabulary_size(),\n","                                  output_dim=UNITS, mask_zero=True)\n","exTarEmbed = embed(exTarIn)\n","\n","result = attentionLayer(exTarEmbed, exContext)\n","\n","print(f'Context sequence, shape (batch, s, units): {exContext.shape}')\n","print(f'Target sequence, shape (batch, t, units): {exTarEmbed.shape}')\n","print(f'Attention result, shape (batch, t, units): {result.shape}')\n","print(f'Attention weights, shape (batch, t, s):    {attentionLayer.last_attention_weights.shape}')"],"metadata":{"id":"BHWclE94iDqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionLayer.last_attention_weights[0].numpy().sum(axis=-1)"],"metadata":{"id":"Bt3rmWmYiDqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionWeights = attentionLayer.last_attention_weights\n","mask=(exContextTok != 0).numpy()\n","\n","plt.subplot(1, 2, 1)\n","plt.pcolormesh(mask*attentionWeights[:, 0, :])\n","plt.title('Attention weights')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(mask)\n","plt.title('Mask');\n"],"metadata":{"id":"wcdOkfxViDqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given the context and target tokens, for each target token it predicts the next target token."],"metadata":{"id":"_6pyXO10iDqB"}},{"cell_type":"markdown","source":["Decoder check"],"metadata":{"id":"yiEpHbwDiDqB"}},{"cell_type":"code","source":["logits = decoder(exContext, exTarIn)\n","\n","print(f'encoder output shape: (batch, s, units) {exContext.shape}')\n","print(f'input target tokens shape: (batch, t) {exTarIn.shape}')\n","print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"],"metadata":{"id":"_L2l8GeSiDqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generation Loop"],"metadata":{"id":"XKaqpeLfiDqC"}},{"cell_type":"code","source":["# Setup the loop variables.\n","nextToken, done, state = decoder.get_initial_state(exContext)\n","tokens = []\n","\n","for n in range(10):\n","  # Run one step.\n","  next_token, done, state = decoder.get_next_token(\n","      exContext, nextToken, done, state, temperature=1.0)\n","  # Add the token to the output.\n","  tokens.append(next_token)\n","\n","# Stack all the tokens together.\n","tokens = tf.concat(tokens, axis=-1) # (batch, t)\n","\n","# Convert the tokens back to a a string\n","result = decoder.tokens_to_text(tokens)\n","result[:3].numpy()"],"metadata":{"id":"gm_5e0muiDqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actual Model"],"metadata":{"id":"ZhMjgjQJiDqC"}},{"cell_type":"code","source":["modelUr = Translator(UNITS, contextTextProcessor, targetTextProcessor)\n","\n","logits = modelUr((exContextTok, exTarIn))\n","\n","print(f'Context tokens, shape: (batch, s, units) {exContextTok.shape}')\n","print(f'Target tokens, shape: (batch, t) {exTarIn.shape}')\n","print(f'logits, shape: (batch, t, targetVocabularySize) {logits.shape}')"],"metadata":{"id":"e1I7gYDqiDqD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"E-2GXCxIiDqD"}},{"cell_type":"markdown","source":["Model Config with \n","Adam optimizer and metrics"],"metadata":{"id":"aKl4_GXhiDqE"}},{"cell_type":"code","source":["modelUr.compile(optimizer='adam',\n","              loss=masked_loss, \n","              metrics=[masked_acc, masked_loss])"],"metadata":{"id":"gKAzKEGQiDqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabSize = 1.0 * targetTextProcessor.vocabulary_size()\n","\n","{\"expected_loss\": tf.math.log(vocabSize).numpy(),\n"," \"expected_acc\": 1/vocabSize}"],"metadata":{"id":"HO7ipWo6iDqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelUr.evaluate(valDS, steps=20, return_dict=True)"],"metadata":{"id":"Do9bMuUWiDqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TRAINING THE MODEL (FINALLY)"],"metadata":{"id":"-FsyzHPiiDqE"}},{"cell_type":"code","source":["historyUr = modelUr.fit(\n","    trainDS.repeat(), \n","    epochs=40,\n","    steps_per_epoch = 130,\n","    validation_data=valDS,\n","    validation_steps = 30,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(patience=3)])"],"metadata":{"id":"IrieUfUwiDqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(historyUr.history['loss'], label='loss')\n","plt.plot(historyUr.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"],"metadata":{"id":"bGRfbrXTiDqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(historyUr.history['masked_acc'], label='accuracy')\n","plt.plot(historyUr.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"],"metadata":{"id":"koCdeJKjiDqF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Translate"],"metadata":{"id":"UnKOmls7iDqF"}},{"cell_type":"code","source":["result = modelUr.translate(['وہ ایک وکیل ہے۔'])\n","result[0].numpy().decode()"],"metadata":{"id":"TV1HWbn0iDqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = modelUr.translate(['وہ ایک پائلٹ ہے۔'])\n","result[0].numpy().decode()"],"metadata":{"id":"VLpxYHM1iDqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result = modelUr.translate(['وہ پائلٹ ہو گی۔'])\n","result[0].numpy().decode()"],"metadata":{"id":"W_eBIGrkiDqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = modelUr.translate(['وہ کوچ ہو گی۔'])\n","result[0].numpy().decode()"],"metadata":{"id":"b1CIk4XWiDqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = modelUr.translate(['میں نے کل اس آدمی کو دیکھا۔'])\n","result[0].numpy().decode()"],"metadata":{"id":"9YLSgMGKiDqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBjoA8w2iDqG"},"outputs":[],"source":["modelUr.plot_attention('وہ ایک وکیل ہے۔')"]},{"cell_type":"markdown","source":["###Bias Performance check"],"metadata":{"id":"iTpOzKCViDqH"}},{"cell_type":"code","source":["pakoccupations=pakoccupations.drop('WikipediaArticle', axis=1)"],"metadata":{"id":"eeIYpB62iDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.head(20)"],"metadata":{"id":"ectaCyYfiDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.columns"],"metadata":{"id":"jWQBxgWliDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.insert(10, \"ModelTranslation(SingleSentence)\", \"\")"],"metadata":{"id":"YetqhditiDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.columns"],"metadata":{"id":"6FOfXI4CZoTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for sent in pakoccupations['Urdu(Sent)']:\n","  result = modelUr.translate([sent])\n","  result=result[0].numpy().decode()\n","  pakoccupations['ModelTranslation(SingleSentence)'].values[ind]=result\n","  ind=ind+1"],"metadata":{"id":"TrLMxXAIiDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.insert(10, \"ModelTranslation(MultipleSentences)\", \"\")"],"metadata":{"id":"cIeKqQEDiDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for sent in pakoccupations['Urdu(SentWithCont)']:\n","  result = modelUr.translate([sent])\n","  result=result[0].numpy().decode()\n","  pakoccupations['ModelTranslation(MultipleSentences)'].values[ind]=result\n","  ind=ind+1"],"metadata":{"id":"QOF4loNxiDqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.insert(10, \"ModelPronoun(SingleSentence)\", \"\")"],"metadata":{"id":"8HBM-9VBiDqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for val in pakoccupations['ModelTranslation(SingleSentence)']:\n","  #print(ind, val)\n","  if (' She ' in val) or ((\" She's \" in val)) or (' she ' in val) or ((\" she's \" in val)):\n","    pakoccupations['ModelPronoun(SingleSentence)'].values[ind]='F'\n","  elif (' He ' in val) or ((\" He's \" in val)) or (' he ' in val) or ((\" he's \" in val)):\n","    pakoccupations['ModelPronoun(SingleSentence)'].values[ind]='M'\n","  else:\n","    pakoccupations['ModelPronoun(SingleSentence)'].values[ind]='O'\n","  ind=ind+1"],"metadata":{"id":"F4moPmDMiDqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.insert(10, \"ModelPronoun(MultipleSentences)\", \"\")"],"metadata":{"id":"BQqNl_ediDqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for val in pakoccupations['ModelTranslation(SingleSentence)']:\n","  #print(ind, val)\n","  if ('She ' in val) or ((\"She's \" in val)) or ('she ' in val) or ((\"she's \" in val)):\n","    pakoccupations['ModelPronoun(SingleSentence)'].values[ind]='F'\n","  elif ('He ' in val) or ((\"He's \" in val)) or ('he ' in val) or ((\"he's \" in val)):\n","    pakoccupations['ModelPronoun(SingleSentence)'].values[ind]='M'\n","  else:\n","    pakoccupations['ModelPronoun(SingleSentence)'].values[ind]='O'\n","  ind=ind+1"],"metadata":{"id":"9EbLaFXkiDqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations_ax=pakoccupations['ModelPronoun(SingleSentence)'].value_counts().plot(kind='pie', autopct=autopct, title='Male, Female and Other pronouns count in the translation without context(Urdu)')\n","plt.show()"],"metadata":{"id":"OnxtVk4aiDqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations['ModelPronoun(SingleSentence)'].value_counts()"],"metadata":{"id":"jUtgo1ZFiDqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations_ax=pakoccupations['ModelPronoun(MultipleSentences)'].value_counts().plot(kind='pie', autopct=autopct, title='Male, Female and Other pronouns count in the translation with context(Bangla)')\n","plt.show()"],"metadata":{"id":"BIzmTNIxiDqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations['ModelPronoun(MultipleSentences)'].value_counts()"],"metadata":{"id":"986QXF05iDqJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Dari/Farsi"],"metadata":{"id":"wwV8Sl3LP6Dd"}},{"cell_type":"markdown","source":["###Getting the context-target pair"],"metadata":{"id":"XKx09eemP6Dd"}},{"cell_type":"code","source":["targetRaw, contextRaw = load_data('/content/drive/MyDrive/Pattern Recognition/MultilingualCorpora/RandomNWiki/combinedWikiRandEn-Fa.txt')\n","print(contextRaw[-1])"],"metadata":{"id":"hUrWv7_SP6Dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(targetRaw[-1])"],"metadata":{"id":"YX0OOGfSP6Dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test Train split"],"metadata":{"id":"toafhiR0P6Dd"}},{"cell_type":"code","source":["BUFFER_SIZE = len(contextRaw)\n","BATCH_SIZE = 64\n","\n","isTrain = np.random.uniform(size=(len(targetRaw),)) < 0.8\n","\n","trainRaw = (\n","    tf.data.Dataset\n","    .from_tensor_slices((contextRaw[isTrainDa], targetRaw[isTrainDa]))\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE))\n","valraw = (\n","    tf.data.Dataset\n","    .from_tensor_slices((contextRaw[~isTrainDa], targetRaw[~isTrainDa]))\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE))"],"metadata":{"id":"NNiwfYQpP6Dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for exampleContextStrings, exampleTargetStrings in trainRawDa.take(1):\n","  print(exampleContextStrings[:5])\n","  print()\n","  print(exampleTargetStrings[:5])\n","  break"],"metadata":{"id":"-UO2zt7UP6De"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Text Preprocessing"],"metadata":{"id":"InLwGkexP6De"}},{"cell_type":"markdown","source":["###Standardization"],"metadata":{"id":"b7MOSkryP6De"}},{"cell_type":"code","source":["exampleText = tf.constant('وجود دارد.')\n","\n","print(exampleText.numpy())\n","print(tf_text.normalize_utf8(exampleText, 'NFKC').numpy())"],"metadata":{"id":"Nsm7VQx-P6De"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unicode Normalization (For all languages)"],"metadata":{"id":"szfLcSPBP6De"}},{"cell_type":"code","source":["print(exampleText.numpy().decode())\n","print(lowerSplitPunct(exampleText).numpy().decode())"],"metadata":{"id":"8blW55zgP6De"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Text Vectorization"],"metadata":{"id":"AAhaFCC-P6De"}},{"cell_type":"markdown","source":["Train (adapt=~  Model.fit)"],"metadata":{"id":"_7RbBISnP6De"}},{"cell_type":"code","source":["contextTextProcessor.adapt(trainRawDa.map(lambda context, target: context))\n","\n","# Here are the first 10 words from the vocabulary:\n","contextTextProcessor.get_vocabulary()[:10]"],"metadata":{"id":"6pLJeKhNP6Df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FOR ENGLISH"],"metadata":{"id":"KRDkdmubP6Df"}},{"cell_type":"code","source":["targetTextProcessor.adapt(trainRawDa.map(lambda context, target: target))\n","targetTextProcessor.get_vocabulary()[:10]"],"metadata":{"id":"c8GuYM9BP6Df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strings batch to tokens"],"metadata":{"id":"As01G4OvP6Df"}},{"cell_type":"code","source":["exampleTokens = contextTextProcessor(exampleContextStrings)\n","exampleTokens[:3, :]"],"metadata":{"id":"INZImMtNP6Df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 2, 1)\n","plt.pcolormesh(exampleTokens.to_tensor())\n","plt.title('Token IDs')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(exampleTokens.to_tensor() != 0)\n","plt.title('Mask')"],"metadata":{"id":"Yenq4r72P6Df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Dataset processing"],"metadata":{"id":"0nGSOdsJP6Df"}},{"cell_type":"markdown","source":["Vectorization"],"metadata":{"id":"7P1CdURGP6Dg"}},{"cell_type":"code","source":["trainDS= trainRaw.map(processText, tf.data.AUTOTUNE)\n","valDS= valraw.map(processText, tf.data.AUTOTUNE)"],"metadata":{"id":"qe3UljlVP6Dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for (exContextTok, exTarIn), exTarOut in trainDS.take(1):\n","  print(exContextTok[0, :10].numpy()) \n","  print()\n","  print(exTarIn[0, :10].numpy()) \n","  print(exTarOut[0, :10].numpy()) "],"metadata":{"id":"yOj8Qbv2P6Dg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder-Decoder"],"metadata":{"id":"IcoU3gMOP6Dg"}},{"cell_type":"markdown","source":["Encoder check"],"metadata":{"id":"AS9JDDI1P6Dg"}},{"cell_type":"code","source":["# Encode the input sequence.\n","encoder = Encoder(contextTextProcessor, UNITS)\n","exContext = encoder(exContextTok)\n","\n","print(f'Context tokens, shape (batch, s): {exContextTok.shape}')\n","print(f'Encoder output, shape (batch, s, units): {exContext.shape}')"],"metadata":{"id":"2vQtdmJtP6Dg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention Layer"],"metadata":{"id":"zUuoSIC0P6Dg"}},{"cell_type":"code","source":["'''attentionLayer = CrossAttention(UNITS)\n","\n","# Attend to the encoded tokens\n","embed = tf.keras.layers.Embedding(targetTextProcessor.vocabulary_size(),\n","                                  output_dim=UNITS, mask_zero=True)'''\n","exTarEmbed = embed(exTarIn)\n","\n","result = attentionLayer(exTarEmbed, exContext)\n","\n","print(f'Context sequence, shape (batch, s, units): {exContext.shape}')\n","print(f'Target sequence, shape (batch, t, units): {exTarEmbed.shape}')\n","print(f'Attention result, shape (batch, t, units): {result.shape}')\n","print(f'Attention weights, shape (batch, t, s):    {attentionLayer.last_attention_weights.shape}')"],"metadata":{"id":"orYJGcgzP6Dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionLayer.last_attention_weights[0].numpy().sum(axis=-1)"],"metadata":{"id":"UaKVoQfgP6Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attentionWeights = attentionLayer.last_attention_weights\n","mask=(exContextTok != 0).numpy()\n","\n","plt.subplot(1, 2, 1)\n","plt.pcolormesh(mask*attentionWeights[:, 0, :])\n","plt.title('Attention weights')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(mask)\n","plt.title('Mask');\n"],"metadata":{"id":"WDvzeMJdP6Dh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given the context and target tokens, for each target token it predicts the next target token."],"metadata":{"id":"XHJgrW63P6Dh"}},{"cell_type":"markdown","source":["Decoder check"],"metadata":{"id":"12kSfeavP6Dh"}},{"cell_type":"code","source":["logits = decoder(exContext, exTarIn)\n","\n","print(f'encoder output shape: (batch, s, units) {exContext.shape}')\n","print(f'input target tokens shape: (batch, t) {exTarIn.shape}')\n","print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"],"metadata":{"id":"gqmk37NBP6Dh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generation Loop"],"metadata":{"id":"NwulBPLiP6Dh"}},{"cell_type":"code","source":["# Setup the loop variables.\n","nextToken, done, state = decoder.get_initial_state(exContext)\n","tokens = []\n","\n","for n in range(10):\n","  # Run one step.\n","  next_token, done, state = decoder.get_next_token(\n","      exContext, nextToken, done, state, temperature=1.0)\n","  # Add the token to the output.\n","  tokens.append(next_token)\n","\n","# Stack all the tokens together.\n","tokens = tf.concat(tokens, axis=-1) # (batch, t)\n","\n","# Convert the tokens back to a a string\n","result = decoder.tokens_to_text(tokens)\n","result[:3].numpy()"],"metadata":{"id":"WKXLLbHSP6Dh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actual Model"],"metadata":{"id":"E3P40PsTP6Dh"}},{"cell_type":"code","source":["modelDa = Translator(UNITS, contextTextProcessor, targetTextProcessor)\n","\n","logits = modelDa((exContextTok, exTarIn))\n","\n","print(f'Context tokens, shape: (batch, s, units) {exContextTok.shape}')\n","print(f'Target tokens, shape: (batch, t) {exTarIn.shape}')\n","print(f'logits, shape: (batch, t, targetVocabularySize) {logits.shape}')"],"metadata":{"id":"_5SBTq8IP6Dh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"WfBizjvBP6Di"}},{"cell_type":"markdown","source":["Model Config with \n","Adam optimizer and metrics"],"metadata":{"id":"8jD3s4ExP6Di"}},{"cell_type":"code","source":["modelDa.compile(optimizer='adam',\n","              loss=masked_loss, \n","              metrics=[masked_acc, masked_loss])"],"metadata":{"id":"cehkaSQlP6Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabSize = 1.0 * targetTextProcessor.vocabulary_size()\n","\n","{\"expected_loss\": tf.math.log(vocabSize).numpy(),\n"," \"expected_acc\": 1/vocabSize}"],"metadata":{"id":"NktyLbIeP6Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelDa.evaluate(valDS, steps=20, return_dict=True)"],"metadata":{"id":"PlzwDosRP6Di"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TRAINING THE MODEL (FINALLY)"],"metadata":{"id":"UTFj2jTHP6Di"}},{"cell_type":"code","source":["historyDa = modelDa.fit(\n","    trainDS.repeat(), \n","    epochs=40,\n","    steps_per_epoch = 130,\n","    validation_data=valDS,\n","    validation_steps = 20,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(patience=3)])"],"metadata":{"id":"96fvpuyFP6Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(historyDa.historyDa['loss'], label='loss')\n","plt.plot(historyDa.historyDa['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"],"metadata":{"id":"Bm_kWS-0P6Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(historyDa.historyDa['masked_acc'], label='accuracy')\n","plt.plot(historyDa.historyDa['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"],"metadata":{"id":"70LJ4yjaP6Dj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Translate"],"metadata":{"id":"O1Hcq_wfP6Dj"}},{"cell_type":"code","source":["result = modelDa.translate(['او یک وکیل است'])\n","result[0].numpy().decode()"],"metadata":{"id":"Ep355N3TP6Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = modelDa.translate(['او یک خلبان است'])\n","result[0].numpy().decode()"],"metadata":{"id":"iGv197azP6Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result = modelDa.translate(['او مربی خواهد شد.'])\n","result[0].numpy().decode()"],"metadata":{"id":"YsI87VtFP6Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = modelDa.translate(['دیروز آن مرد را دیدم.'])\n","result[0].numpy().decode()"],"metadata":{"id":"860GOhsWP6Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leWJWCFXP6Dj"},"outputs":[],"source":["modelUr.plot_attention('او یک خلبان است.')"]},{"cell_type":"markdown","source":["###Bias Performance check"],"metadata":{"id":"FkKR8ZScP6Dk"}},{"cell_type":"code","source":["afgoccupations=afgoccupations.drop('WikipediaArticle', axis=1)"],"metadata":{"id":"oVvTGKEjP6Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations.head()"],"metadata":{"id":"l8HX39TnP6Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations.columns"],"metadata":{"id":"Q16Rep9sP6Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations.insert(10, \"ModelTranslation(SingleSentence)\", \"\")"],"metadata":{"id":"otFfNuyfP6Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for sent in afgoccupations['Sent(Bn)']:\n","  result = modelDa.translate([sent])\n","  result=result[0].numpy().decode()\n","  afgoccupations['ModelTranslation(SingleSentence)'].values[ind]=result\n","  ind=ind+1"],"metadata":{"id":"vjy0dQROP6Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pakoccupations.insert(10, \"ModelTranslation(MultipleSentences)\", \"\")"],"metadata":{"id":"dGyLtyuHP6Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations.columns"],"metadata":{"id":"HdEuH727ZZJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for sent in afgoccupations['SentWithCont(Bn)']:\n","  result = modelDa.translate([sent])\n","  result=result[0].numpy().decode()\n","  pakoccupations['ModelTranslation(MultipleSentences)'].values[ind]=result\n","  ind=ind+1"],"metadata":{"id":"FhMNUu-NP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations.insert(10, \"ModelPronoun(SingleSentence)\", \"\")"],"metadata":{"id":"9Jh0No2RP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for val in afgoccupations['ModelTranslation(SingleSentence)']:\n","  #print(ind, val)\n","  if (' She ' in val) or ((\" She's \" in val)) or (' she ' in val) or ((\" she's \" in val)):\n","    afgoccupations['ModelPronoun(SingleSentence)'].values[ind]='F'\n","  elif (' He ' in val) or ((\" He's \" in val)) or (' he ' in val) or ((\" he's \" in val)):\n","    afgoccupations['ModelPronoun(SingleSentence)'].values[ind]='M'\n","  else:\n","    afgoccupations['ModelPronoun(SingleSentence)'].values[ind]='O'\n","  ind=ind+1"],"metadata":{"id":"0A0-g3LJP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations.insert(10, \"ModelPronoun(MultipleSentences)\", \"\")"],"metadata":{"id":"2i8vy8NZP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ind=0\n","for val in afgoccupations['ModelTranslation(SingleSentence)']:\n","  #print(ind, val)\n","  if ('She ' in val) or ((\"She's \" in val)) or ('she ' in val) or ((\"she's \" in val)):\n","    afgoccupations['ModelPronoun(SingleSentence)'].values[ind]='F'\n","  elif ('He ' in val) or ((\"He's \" in val)) or ('he ' in val) or ((\"he's \" in val)):\n","    afgoccupations['ModelPronoun(SingleSentence)'].values[ind]='M'\n","  else:\n","    afgoccupations['ModelPronoun(SingleSentence)'].values[ind]='O'\n","  ind=ind+1"],"metadata":{"id":"dxqk4ERcP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations_ax=afgoccupations['ModelPronoun(SingleSentence)'].value_counts().plot(kind='pie', autopct=autopct, title='Male, Female and Other pronouns count in the translation without context(Bangla)')\n","plt.show()"],"metadata":{"id":"zC2wk4ndP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations['ModelPronoun(SingleSentence)'].value_counts()"],"metadata":{"id":"r7xJad79P6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations_ax=afgoccupations['ModelPronoun(MultipleSentences)'].value_counts().plot(kind='pie', autopct=autopct, title='Male, Female and Other pronouns count in the translation with context(Bangla)')\n","plt.show()"],"metadata":{"id":"bXl9tyPuP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afgoccupations['ModelPronoun(MultipleSentences)'].value_counts()"],"metadata":{"id":"iwdLdprbP6Dl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#BLEU Metrics"],"metadata":{"id":"YWtAvJaJQAi-"}},{"metadata":{"id":"_arqa6LRnzCL"},"cell_type":"markdown","source":["### 1. Libraries\n","*Install and import necessary libraries*\n"]},{"metadata":{"id":"xFOnk5JdnuYQ"},"cell_type":"code","source":["import nltk\n","import nltk.translate.bleu_score as bleu\n","\n","import math\n","import numpy\n","import os\n","\n","try:\n","  nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","  nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"SVkfsYSZq_zn"},"cell_type":"markdown","source":["### 2. Dataset\n","*Array of words: candidate and reference sentences split into words*"]},{"metadata":{"id":"Dr9v92X0r9VM"},"cell_type":"code","source":["hyp = str('she read the book because she was interested in world history').split()\n","ref_a = str('she read the book because she was interested in world history').split()\n","ref_b = str('she was interested in world history because she read the book').split()"],"execution_count":null,"outputs":[]}]}